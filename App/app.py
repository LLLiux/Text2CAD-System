import os
import sys
import time
import torch
import gradio as gr
import trimesh
import yaml
import shutil
from zhipuai import ZhipuAI

sys.path.append("..")
sys.path.append("/".join(os.path.abspath(__file__).split("/")[:-1]))
sys.path.append("/".join(os.path.abspath(__file__).split("/")[:-2]))

from Cad_VLM.models.text2cad import Text2CAD
from CadSeqProc.utility.macro import MAX_CAD_SEQUENCE_LENGTH, N_BIT
from CadSeqProc.cad_sequence import CADSequence

# é…ç½®å¸¸é‡
OUTPUT_DIR = os.path.abspath("output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å…¨å±€çŠ¶æ€ç®¡ç†
class AppState:
    def __init__(self):
        self.original_path = None
        self.base_scale = [1.0, 1.0, 1.0]
        self.current_preview = None
        self.latest_scale = [1.0, 1.0, 1.0]

app_state = AppState()

# æ¨¡å‹åŠ è½½
def load_model(config_path, device):
    with open(config_path) as f:
        config = yaml.safe_load(f)
    
    cad_config = config["cad_decoder"]
    cad_config["cad_seq_len"] = MAX_CAD_SEQUENCE_LENGTH
    
    model = Text2CAD(
        text_config=config["text_encoder"],
        cad_config=cad_config
    ).to(device)
    
    if config["test"]["checkpoint_path"]:
        checkpoint = torch.load(config["test"]["checkpoint_path"], map_location=device)
        pretrained_dict = {k.replace("module.", ""): v for k, v in checkpoint["model_state_dict"].items()}
        model.load_state_dict(pretrained_dict, strict=False)
    
    model.eval()
    print("æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = load_model("../Cad_VLM/config/inference_user_input.yaml", device)

# æ ¸å¿ƒç”Ÿæˆå‡½æ•°
def test_model(text):
    try:
        pred_dict = model.test_decode(
            texts=[text] if not isinstance(text, list) else text,
            maxlen=MAX_CAD_SEQUENCE_LENGTH,
            nucleus_prob=0,
            topk_index=1,
            device=device
        )
        
        cad_seq = CADSequence.from_vec(
            pred_dict["cad_vec"][0].cpu().numpy(),
            bit=N_BIT,
            post_processing=True
        )
        return cad_seq.create_mesh().mesh, cad_seq
    except Exception as e:
        print(f"Model error: {str(e)}")
        return None, str(e)

# æ–‡ä»¶ç®¡ç†
def clean_files(max_keep=5):
    try:
        files = sorted(os.listdir(OUTPUT_DIR), 
                     key=lambda x: os.path.getctime(os.path.join(OUTPUT_DIR, x)))
        for f in files[:-max_keep]:
            os.remove(os.path.join(OUTPUT_DIR, f))
    except Exception as e:
        print(f"Clean error: {str(e)}")

# ç¿»è¯‘
def translate(text):
    prompt = "ä½ æ˜¯ä¸€åç²¾é€šä¸­è‹±åŒè¯­ä¸”å…·æœ‰æœºæ¢°å·¥ç¨‹èƒŒæ™¯çš„ä¸“ä¸šç¿»è¯‘å®˜ï¼Œä¸“é—¨ä¸ºCADè®¾è®¡é¢†åŸŸæä¾›æœ¯è¯­ç²¾å‡†çš„ç¿»è¯‘æœåŠ¡ã€‚æˆ‘å°†æä¾›ä¸­æ–‡ï¼Œä½ è´Ÿè´£å°†å…¶ç¿»è¯‘æˆè‹±æ–‡ï¼Œå¹¶ä»¥åè¯å½¢å¼è¿”å›ã€‚åªéœ€è¦è¿”å›ä¸€ä¸ªè‹±æ–‡åè¯ã€‚"
    client = ZhipuAI(api_key="00244246da7e40b68b4bf6522b72c023.jRs7WuZF9HQ9Y0LG")
    response = client.chat.completions.create(
        model="glm-4-plus",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": text}
        ],
    )
    translated = response.choices[0].message.content
    print(translated)
    return translated

# ä¸»ç”Ÿæˆæµç¨‹
def generate_model(text, scale_x=1.0, scale_y=1.0, scale_z=1.0):
    global app_state
    try:
        clean_files()
        translated = translate(text)
        
        # ç”ŸæˆåŸºç¡€æ¨¡å‹
        mesh, cad = test_model(translated)
        print("ç”ŸæˆæˆåŠŸ")
        if not mesh:
            return None, None, f"ç”Ÿæˆå¤±è´¥ï¼š{cad}"
        
        # ä¿å­˜åŸå§‹æ¨¡å‹
        timestamp = int(time.time())
        original_path = os.path.join(OUTPUT_DIR, f"original_{timestamp}.stl")
        mesh.export(original_path)
        app_state.original_path = original_path
        app_state.base_scale = [scale_x, scale_y, scale_z]
        
        # ç”Ÿæˆåˆå§‹ç¼©æ”¾ç‰ˆæœ¬
        scaled = mesh.apply_scale(app_state.base_scale)
        output_path = os.path.join(OUTPUT_DIR, f"output_{timestamp}.stl")
        scaled.export(output_path)
        app_state.current_preview = output_path
        app_state.latest_scale = [1.0, 1.0, 1.0]
        
        print(f"ç”ŸæˆæˆåŠŸï¼ŒåŸå§‹æ–‡ä»¶ï¼š{original_path}")
        return output_path, output_path, output_path
        
    except Exception as e:
        print(f"Generate error: {str(e)}")
        return None, None, f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}"

# å®æ—¶è°ƒæ•´å‡½æ•°
def update_preview(live_x, live_y, live_z):
    try:
        print(f"\n=== è°ƒæ•´è§¦å‘ ===")
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        print(f"ğŸ”§ åŠ è½½åŸå§‹æ¨¡å‹ï¼š{app_state.original_path}")
        original = trimesh.load(app_state.original_path)
        
        # åº”ç”¨å®æ—¶ç¼©æ”¾
        actual_scale = [
            app_state.base_scale[0] * live_x,
            app_state.base_scale[1] * live_y,
            app_state.base_scale[2] * live_z
        ]
        print(f"âš–ï¸ åº”ç”¨ç¼©æ”¾æ¯”ä¾‹ï¼š{actual_scale}")
        scaled = original.apply_scale(actual_scale)
        
        # ç”Ÿæˆæ–°é¢„è§ˆæ–‡ä»¶
        new_filename = f"preview_live_{int(time.time())}.stl"
        preview_path = os.path.join(OUTPUT_DIR, new_filename)
        scaled.export(preview_path)
        print(f"ğŸ’¾ ä¿å­˜é¢„è§ˆæ–‡ä»¶ï¼š{preview_path}")
        
        return preview_path, preview_path
    except Exception as e:
        print(f"å®æ—¶è°ƒæ•´å¤±è´¥: {str(e)}")
        return app_state.original_path  # è¿”å›åŸå§‹è·¯å¾„ä¿è¯æ˜¾ç¤ºä¸ä¸­æ–­

# Gradioç•Œé¢
with gr.Blocks(theme=gr.themes.Soft(), css="footer {visibility: hidden}") as demo:
    gr.Markdown("# Text2CAD")
    
    with gr.Row():
        input_col = gr.Column()
        output_col = gr.Column()
        
        with input_col:
            text_input = gr.Textbox(label="è®¾è®¡æè¿°", 
                                  placeholder="ç¤ºä¾‹ï¼šä¸€ä¸ªä¸­å¿ƒå¸¦é€šå­”çš„é‡‘å±æ”¯æ¶...",
                                  max_lines=3)
            
            with gr.Accordion("ğŸ”§ åˆå§‹æ¯”ä¾‹è®¾ç½®", open=False):
                init_scale_x = gr.Slider(0.1, 5.0, 1.0, step=0.1, label="Xè½´")
                init_scale_y = gr.Slider(0.1, 5.0, 1.0, step=0.1, label="Yè½´")
                init_scale_z = gr.Slider(0.1, 5.0, 1.0, step=0.1, label="Zè½´")
                
            generate_btn = gr.Button("ğŸš€ ç”Ÿæˆæ¨¡å‹", variant="primary")
            
            with gr.Accordion("ğŸ”§ å®æ—¶è°ƒæ•´å·¥å…·", open=False):
                live_scale_x = gr.Slider(0.1, 5.0, 1.0, step=0.1, label="Xè½´å¾®è°ƒ")
                live_scale_y = gr.Slider(0.1, 5.0, 1.0, step=0.1, label="Yè½´å¾®è°ƒ")
                live_scale_z = gr.Slider(0.1, 5.0, 1.0, step=0.1, label="Zè½´å¾®è°ƒ")
                gr.Markdown("**æç¤º**ï¼šè°ƒæ•´åæ¾å¼€æ»‘å—å³å¯æŸ¥çœ‹æ•ˆæœ")
            
        with output_col:
            model_view = gr.Model3D(
                label="ä¸‰ç»´é¢„è§ˆ",
                height=500,
                zoom_speed=1.5,
                # å¼ºåˆ¶æ¯æ¬¡æ›´æ–°éƒ½é‡æ–°åŠ è½½æ¨¡å‹
                every=0.1,
                interactive=True,
                # æ·»åŠ è‡ªå®šä¹‰ç¼“å­˜æ¸…é™¤å‚æ•°
                elem_id="live_preview"
            )
            original = gr.File(label="ä¸‹è½½åŸå§‹æ¨¡å‹")
            scaled = gr.File(label="ä¸‹è½½ç¼©æ”¾åçš„æ¨¡å‹")

    examples = gr.Examples(
        examples=[
            ["ä¸€ä¸ªé‡‘å±ç¯", 1.0, 1.0, 1.0],
            ["ä¸€ä¸ªäº”è§’æ˜Ÿ", 1.5, 1.5, 0.5],
            ["å¸¦å››ä¸ªå®‰è£…å­”çš„çŸ©å½¢æ¿", 1.0, 0.8, 1.2],
            ["åœ†æŸ±ä½“ä¸­å¿ƒæœ‰é€šå­”çš„ç»“æ„", 1.0, 1.0, 2.0]
        ],
        inputs=[text_input, init_scale_x, init_scale_y, init_scale_z],
        label="ç¤ºä¾‹æç¤º",
        fn=lambda x,y,z,w: generate_model(x,y,z,w)[1],
        run_on_click=True
    )

    # äº‹ä»¶ç»‘å®š
    generate_btn.click(
        fn=generate_model,
        inputs=[text_input, init_scale_x, init_scale_y, init_scale_z],
        outputs=[original, scaled, model_view],
        api_name="generate"
    )
    
    for component in [live_scale_x, live_scale_y, live_scale_z]:
        component.release(
            fn=update_preview,
            inputs=[live_scale_x, live_scale_y, live_scale_z],
            outputs=[scaled, model_view]
        )

if __name__ == "__main__":
    # å…è®¸è·¨åŸŸè®¿é—®ï¼ˆå¼€å‘ç¯å¢ƒéœ€è¦ï¼‰
    os.environ["GRADIO_SERVER_NAME"] = "0.0.0.0"
    os.environ["GRADIO_CACHE"] = "false"
    
    # å¯åŠ¨æ—¶æ¸…ç†æ—§æ–‡ä»¶
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # å¯åŠ¨åº”ç”¨
    demo.launch(
        server_port=7860,
        share=True,
        debug=True,
        show_api=False
    )